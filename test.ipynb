{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ea59bf654b4643916de10b49f34a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"LoftQ/Mistral-7B-v0.1-4bit-64rank\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,  \n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "    ),\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    MODEL_ID,\n",
    "    subfolder=\"loftq_init\",\n",
    "    is_trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocessing_BoolQA.<locals>.<lambda> at 0x7f0a3c1b49d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 15134.68ex/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 16078.75ex/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 16258.88ex/s]\n"
     ]
    }
   ],
   "source": [
    "from datamodule import datamodule\n",
    "\n",
    "#path = [\"/home/elicer/M-LLM/data/BoolQA.csv\", \"/home/elicer/M-LLM/data/NLI_CB.csv\", \"/home/elicer/M-LLM/data/sc_amazon.csv\"]\n",
    "path = \"/home/elicer/M-LLM/data/BoolQA.csv\"\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = datamodule.preprare_dataset(path)\n",
    "# train_dataset = train_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "# val_dataset = val_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "# test_dataset = test_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e3085503ed4109b8da87397d4357cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "text_data = {'text': train_dataset['text']}\n",
    "train_dataset = Dataset.from_dict(text_data)\n",
    "train_dataset = train_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrion_\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.login()\n",
    "os.environ[\"WANDB_PROJECT\"]=\"M-LLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 17:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.951400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.129100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7295bb96af14df0a99cc9ce73e1e13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▄█▄▅▃▅▅▃▇▁▇▄▆▅▆▄▃▆▃▆▅▆▃▄▆▄▅▇▅▄▅▄▃▇▅▄▆▅▅▅</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.1291</td></tr><tr><td>train/total_flos</td><td>1.7327453172006912e+16</td></tr><tr><td>train/train_loss</td><td>2.12541</td></tr><tr><td>train/train_runtime</td><td>1045.7157</td></tr><tr><td>train/train_samples_per_second</td><td>1.913</td></tr><tr><td>train/train_steps_per_second</td><td>0.478</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Mistral-BoolQA</strong> at: <a href='https://wandb.ai/rion_/huggingface/runs/hde5pslq' target=\"_blank\">https://wandb.ai/rion_/huggingface/runs/hde5pslq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240106_134012-hde5pslq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "args = transformers.TrainingArguments(\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"sgd\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"Mistral-BoolQA\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "peft_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c9bd6592454bc58faff1b59622176c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/JD97/BoolQA/commit/bf9d1facb2ece0a13276581f84d8e564f9cbe7cd', commit_message='Upload model', commit_description='', oid='bf9d1facb2ece0a13276581f84d8e564f9cbe7cd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.push_to_hub(\"JD97/BoolQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lora_reassign_weights(model, state_dict, r, lora_alpha, fan_in_fan_out=False, merge=True):\n",
    "    is_merged = getattr(model, \"is_merged\", False)\n",
    "    assert is_merged != merge, f'{is_merged} != {merge}: if is_merged, then must be unmerge; if not is_merged, then must merge'\n",
    "    named_params = [(n, p) for n, p in model.named_parameters()]\n",
    "    scaling = lora_alpha / r\n",
    "    print(f'Lora configs: alpha={lora_alpha}, r={r}, scaling={scaling}')\n",
    "    state_dict = {k.replace(\"base_model.model.\", \"\"): v for k, v in state_dict.items()}\n",
    "    replaced = set()\n",
    "    merged_names = {\n",
    "        # these are projector weights that got combined into single matrix in vllm\n",
    "        \"qkv_proj\": [\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "        \"gate_up_proj\": [\"gate_proj\", \"up_proj\"]\n",
    "    }\n",
    "    non_merged_names = ['o_proj', 'down_proj']\n",
    "    for name, param in named_params:\n",
    "        param.requires_grad = False\n",
    "        if \"_proj.weight\" not in name:\n",
    "            continue\n",
    "        for wn, wn_series in merged_names.items():\n",
    "            if name.endswith(f\"{wn}.weight\"):\n",
    "                for stride_id, att_weight_name in enumerate(wn_series):\n",
    "                    lora_a = name.replace(f\"{wn}.weight\", f\"{att_weight_name}.lora_A.weight\")\n",
    "                    lora_b = name.replace(f\"{wn}.weight\", f\"{att_weight_name}.lora_B.weight\")\n",
    "                    shard_size = param.shape[0] // len(wn_series)\n",
    "                    if lora_a in state_dict:\n",
    "                        assert lora_b in state_dict, f'{lora_b} not in state_dict'\n",
    "                        assert state_dict[lora_b].shape[1] == r, f'{r=} != {state_dict[lora_b].shape}'\n",
    "                        matrix = transpose(state_dict[lora_b] @ state_dict[lora_a], fan_in_fan_out) * scaling\n",
    "                        assert param.data[shard_size * stride_id:shard_size * (stride_id + 1)].shape == matrix.shape\n",
    "                        if merge:\n",
    "                            param.data[shard_size * stride_id:shard_size * (stride_id + 1)] += matrix\n",
    "                        else:\n",
    "                            param.data[shard_size * stride_id:shard_size * (stride_id + 1)] -= matrix\n",
    "                        replaced.add(lora_a)\n",
    "                        replaced.add(lora_b)\n",
    "        for wn in non_merged_names:\n",
    "            if name.endswith(f\"{wn}.weight\"):\n",
    "                lora_a = name.replace(f\"{wn}.weight\", f\"{wn}.lora_A.weight\")\n",
    "                lora_b = name.replace(f\"{wn}.weight\", f\"{wn}.lora_B.weight\")\n",
    "                if lora_a in state_dict:\n",
    "                    assert lora_b in state_dict\n",
    "                    matrix = transpose(state_dict[lora_b] @ state_dict[lora_a], fan_in_fan_out) * scaling\n",
    "                    assert param.data.shape == matrix.shape, f'invalid shape: {name} {param.data.shape} != {matrix.shape}'\n",
    "                    if merge:\n",
    "                        param.data += matrix\n",
    "                    else:\n",
    "                        param.data -= matrix\n",
    "                    replaced.add(lora_a)\n",
    "                    replaced.add(lora_b)\n",
    "    no_replaced = [k for k in state_dict.keys() if k not in replaced]\n",
    "    assert len(no_replaced) == 0, f'some lora states not loaded, check again!: {no_replaced}'\n",
    "    model.is_merged = merge\n",
    "\n",
    "\n",
    "def lora_merge_unmerge_state_dict(llm, state_dict, peft_config, merge=True):\n",
    "    # merge lora states to weights\n",
    "    for worker in llm.llm_engine.workers:\n",
    "        lora_reassign_weights(worker.model, state_dict, \n",
    "            r=peft_config.r, \n",
    "            lora_alpha=peft_config.lora_alpha, \n",
    "            fan_in_fan_out=peft_config.fan_in_fan_out, \n",
    "            merge=merge\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 19:09:11,766\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-06 19:09:11 llm_engine.py:70] Initializing an LLM engine with config: model='LoftQ/Mistral-7B-v0.1-4bit-64rank', tokenizer='LoftQ/Mistral-7B-v0.1-4bit-64rank', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-06 19:09:25 llm_engine.py:275] # GPU blocks: 8145, # CPU blocks: 2048\n",
      "INFO 01-06 19:09:27 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-06 19:09:27 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-06 19:09:32 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_id = \"LoftQ/Mistral-7B-v0.1-4bit-64rank\"\n",
    "peft_id = \"JD97/BoolQA\"\n",
    "\n",
    "llm = LLM(model=model_id)\n",
    "adapter_state_dict = load_peft_weights(peft_id)\n",
    "config = PeftConfig(peft_id)\n",
    "lora_merge_unmerge_state_dict(llm, adapter_state_dict, config, merge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ' option:\\n      a. true\\n      b. false\\n      c.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# prompts = [\n",
    "#     \"Hello, my name is\",\n",
    "#     \"The capital of France is\",\n",
    "#     \"The future of AI is\",\n",
    "# ]\n",
    "\n",
    "prompts = [train_dataset[73]['text'][:-15]]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0, top_k=-1)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# for output in outputs:\n",
    "#     prompt = output.prompt\n",
    "#     generated_text = output.outputs[0].text\n",
    "#     print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, load_peft_weights\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# model_id = \"LoftQ/Mistral-7B-v0.1-4bit-64rank\"  \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "# peft_model_id = \"JD97/BoolQA\"\n",
    "# config = PeftConfig.from_pretrained(peft_model_id)\n",
    "# model = PeftModel.from_pretrained(model, peft_model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_state_dict = load_peft_weights(\"JD97/BoolQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_state_dict = torch.load(\"lora_states.pt\")['module']\n",
    "lora_merge_unmerge_state_dict(llm, lora_state_dict, merge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load original llama to vllm with llm = LLM(\"llama-7b\") ...\n",
    "Load lora states dict lora_state_dict = torch.load(\"lora_states.pt\")['module'].\n",
    "Merge lora states to llm do lora_merge_unmerge_state_dict(llm, lora_state_dict, merge=True)\n",
    "Do whatever inference job with llm ...\n",
    "To unmerge and obtain the original llama, run lora_merge_unmerge_state_dict(llm, lora_state_dict, merge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(model=\"facebook/opt-125m\", gpu_memory_utilization=0.05)\n",
    "\n",
    "# Add LoRA adapter\n",
    "lora.LoRAModel.from_pretrained(llm.llm_engine.workers[0].model, \"edbeeching/opt-125m-imdb-lora\")\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0, top_k=-1)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
