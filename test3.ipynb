{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, load_peft_weights\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from task_vector import TaskVector\n",
    "\n",
    "model_id = \"LoftQ/Mistral-7B-v0.1-4bit-64rank\"\n",
    "peft_ids = [\"JD97/BoolQA\", \"JD97/SC\"]\n",
    "\n",
    "task_vectors = [\n",
    "    TaskVector(model_id, peft_id, lora=True)\n",
    "    for peft_id in peft_ids\n",
    "]\n",
    "\n",
    "task_vector_sum = sum(task_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.66s/it]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = task_vector_sum.apply_to_lora(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|██████████| 671M/671M [00:31<00:00, 21.1MB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/JD97/ttt/commit/8c1c6512034ffcc73bca40c5b5e0b891f24608eb', commit_message='Upload model', commit_description='', oid='8c1c6512034ffcc73bca40c5b5e0b891f24608eb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"JD97/ttt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lora_reassign_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/M-LLM/test3.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://jzxregdpdtqntfyh.tunnel-pt.elice.io/home/elicer/M-LLM/test3.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m lora_reassign_weights()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lora_reassign_weights' is not defined"
     ]
    }
   ],
   "source": [
    "lora_reassign_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lora_reassign_weights(model, state_dict, r, lora_alpha, fan_in_fan_out=False, merge=True):\n",
    "    is_merged = getattr(model, \"is_merged\", False)\n",
    "    assert is_merged != merge, f'{is_merged} != {merge}: if is_merged, then must be unmerge; if not is_merged, then must merge'\n",
    "    named_params = [(n, p) for n, p in model.named_parameters()]\n",
    "    scaling = lora_alpha / r\n",
    "    print(f'Lora configs: alpha={lora_alpha}, r={r}, scaling={scaling}')\n",
    "    state_dict = {k.replace(\"base_model.model.\", \"\"): v for k, v in state_dict.items()}\n",
    "    replaced = set()\n",
    "    merged_names = {\n",
    "        # these are projector weights that got combined into single matrix in vllm\n",
    "        \"qkv_proj\": [\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "        \"gate_up_proj\": [\"gate_proj\", \"up_proj\"]\n",
    "    }\n",
    "    non_merged_names = ['o_proj', 'down_proj']\n",
    "    for name, param in named_params:\n",
    "        param.requires_grad = False\n",
    "        if \"_proj.weight\" not in name:\n",
    "            continue\n",
    "        for wn, wn_series in merged_names.items():\n",
    "            if name.endswith(f\"{wn}.weight\"):\n",
    "                for stride_id, att_weight_name in enumerate(wn_series):\n",
    "                    lora_a = name.replace(f\"{wn}.weight\", f\"{att_weight_name}.lora_A.weight\")\n",
    "                    lora_b = name.replace(f\"{wn}.weight\", f\"{att_weight_name}.lora_B.weight\")\n",
    "                    shard_size = param.shape[0] // len(wn_series)\n",
    "                    if lora_a in state_dict:\n",
    "                        assert lora_b in state_dict, f'{lora_b} not in state_dict'\n",
    "                        assert state_dict[lora_b].shape[1] == r, f'{r=} != {state_dict[lora_b].shape}'\n",
    "                        matrix = transpose(state_dict[lora_b] @ state_dict[lora_a], fan_in_fan_out) * scaling\n",
    "                        assert param.data[shard_size * stride_id:shard_size * (stride_id + 1)].shape == matrix.shape\n",
    "                        if merge:\n",
    "                            param.data[shard_size * stride_id:shard_size * (stride_id + 1)] += matrix\n",
    "                        else:\n",
    "                            param.data[shard_size * stride_id:shard_size * (stride_id + 1)] -= matrix\n",
    "                        replaced.add(lora_a)\n",
    "                        replaced.add(lora_b)\n",
    "        for wn in non_merged_names:\n",
    "            if name.endswith(f\"{wn}.weight\"):\n",
    "                lora_a = name.replace(f\"{wn}.weight\", f\"{wn}.lora_A.weight\")\n",
    "                lora_b = name.replace(f\"{wn}.weight\", f\"{wn}.lora_B.weight\")\n",
    "                if lora_a in state_dict:\n",
    "                    assert lora_b in state_dict\n",
    "                    matrix = transpose(state_dict[lora_b] @ state_dict[lora_a], fan_in_fan_out) * scaling\n",
    "                    assert param.data.shape == matrix.shape, f'invalid shape: {name} {param.data.shape} != {matrix.shape}'\n",
    "                    if merge:\n",
    "                        param.data += matrix\n",
    "                    else:\n",
    "                        param.data -= matrix\n",
    "                    replaced.add(lora_a)\n",
    "                    replaced.add(lora_b)\n",
    "    no_replaced = [k for k in state_dict.keys() if k not in replaced]\n",
    "    assert len(no_replaced) == 0, f'some lora states not loaded, check again!: {no_replaced}'\n",
    "    model.is_merged = merge\n",
    "\n",
    "\n",
    "def lora_merge_unmerge_state_dict(llm, state_dict, peft_config, merge=True):\n",
    "    # merge lora states to weights\n",
    "    for worker in llm.llm_engine.workers:\n",
    "        lora_reassign_weights(worker.model, state_dict, \n",
    "            r=peft_config.r, \n",
    "            lora_alpha=peft_config.lora_alpha, \n",
    "            fan_in_fan_out=peft_config.fan_in_fan_out, \n",
    "            merge=merge\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-07 11:24:22,084\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-07 11:24:26 llm_engine.py:70] Initializing an LLM engine with config: model='LoftQ/Mistral-7B-v0.1-4bit-64rank', tokenizer='LoftQ/Mistral-7B-v0.1-4bit-64rank', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 01-07 11:24:40 llm_engine.py:275] # GPU blocks: 8486, # CPU blocks: 2048\n",
      "INFO 01-07 11:24:42 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-07 11:24:42 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-07 11:24:46 model_runner.py:547] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|██████████| 671M/671M [00:25<00:00, 26.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, load_peft_weights \n",
    "\n",
    "model_id = \"LoftQ/Mistral-7B-v0.1-4bit-64rank\"\n",
    "peft_id = \"JD97/ttt\"\n",
    "\n",
    "llm = LLM(model=model_id)\n",
    "adapter_state_dict = load_peft_weights(peft_id)\n",
    "config = PeftConfig(peft_id)\n",
    "lora_merge_unmerge_state_dict(llm, adapter_state_dict, config, merge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocessing_SC.<locals>.<lambda> at 0x7f9ca41a77f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 14687.20ex/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 16221.78ex/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 15915.25ex/s]\n"
     ]
    }
   ],
   "source": [
    "from datamodule import datamodule\n",
    "\n",
    "#path = [\"/home/elicer/M-LLM/data/BoolQA.csv\", \"/home/elicer/M-LLM/data/NLI_CB.csv\", \"/home/elicer/M-LLM/data/sc_amazon.csv\"]\n",
    "path = \"/home/elicer/M-LLM/data/sc_amazon.csv\"\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = datamodule.preprare_dataset(path)\n",
    "# train_dataset = train_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "# val_dataset = val_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "# test_dataset = test_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ' A. Positive\\n        B. Negative\\n        C. Neut'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "prompts = [train_dataset[73]['text'][:-15]]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0, top_k=-1)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# for output in outputs:\n",
    "#     prompt = output.prompt\n",
    "#     generated_text = output.outputs[0].text\n",
    "#     print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"What is the sentiment of the following paragraph? Choose one from the option.\\n \\n        sentence: Title: Maybe I Expected Too Much...\\nText: AN ENGLISH MURDER was chosen (by me) to be read by my book group, after a good review of it in the Cincinnati Enquirer. I was disappointed. I\\'m not a frequent reader of \"English mysteries,\" so I can\\'t compare and contrast this book to all the other mysteries I\\'ve read. It\\'s just that I was left wondering where the \"mystery\" was. The identity of the murderer was obvious from the start. It\\'s not a \"whodunnit.\" Where this book did succeed was in slowly revealing that the citizens of a seemingly pleasant little English village are not as quaint as their surroundings...everyone has dark and often slimy secrets in their past. As the weather turned gloomy and characters\\' conscious and unconscious motives were uncovered, the book just went sort of grim. If you\\'re looking for a classic mystery with dead ends and plot twists, AN ENGLISH MURDER isn\\'t it. If you\\'re looking for a character study of the dark side of people\\'s souls, you\\'ll find those secret corners here.\\n \\n \\n        label: neutral'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[73]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
